from pathlib import Path
from typing import Dict, Set

import pandas as pd
from snakemake.utils import min_version

# require minimum snakemake version
min_version("6.10.0")


# =====================================
# Workflow config items
configfile: "config/config.yaml"


samplesheet = pd.read_csv(config["samplesheet"], index_col="sample")
containers: Dict[str, str] = config["containers"]
# =====================================

# =====================================
# Constants for workflow
GB = 1_024
rules_dir = Path("workflow/rules").resolve()
env_dir = Path("workflow/envs").resolve()
scripts_dir = Path("workflow/scripts").resolve()
fast5_dir = Path(config["fast5_dir"]).resolve()
data_dir = Path("data").resolve()
GUPPY_VERSION = containers["guppy-gpu"].split(":")[-1]
rule_log_dir = Path("logs/stderr")
decontam_db = data_dir / "decontam_db"
results = Path("results").resolve()
filtered_dir = results / "filtered"
subsample_dir = results / "subsample"
plot_dir = results / "plots"
amr_dir = results / "amr_predictions"
report_dir = results / "report"
# =====================================


# =====================================
# Generate the set of required output files from the pipeline
output: Set[Path] = set()
filter_logfiles: Set[Path] = set()
mykrobe_results: Set[Path] = set()
rasusa_logs: Set[Path] = set()
for sample, row in samplesheet.iterrows():
    output.add(data_dir / f"fastqs/guppy_v{GUPPY_VERSION}/{sample}.fq.gz")
    filter_logfiles.add(rule_log_dir / "filter_contamination/{sample}.log")
    mykrobe_results.add(amr_dir / f"{sample}.mykrobe.json")
    rasusa_logs.add(rule_log_dir / f"subsample_reads/{sample}.log")
output.add(report_dir / "composition.html")
output.add(report_dir / "amr.html")
# =====================================


rule all:
    input:
        output,


include: rules_dir / "basecall.smk"
include: rules_dir / "qc.smk"
include: rules_dir / "amr.smk"
include: rules_dir / "report.smk"
